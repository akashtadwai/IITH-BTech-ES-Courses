# -*- coding: utf-8 -*-
"""trainingpart (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p441TL4qIB9A50nZdSbm2QCRB_YkI8NF

## Importing libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# Import some necessary librairies
import numpy as np 
import pandas as pd 
# %matplotlib inline
import matplotlib.pyplot as plt  
import seaborn as sns
import pickle
from scipy import stats
from subprocess import check_output
color = sns.color_palette()
sns.set_style('darkgrid')

# To ignore the warning messages
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn 

pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting float output to 3 decimal points

"""## Loading the dataset"""

train = pd.read_csv('./train_input.csv') # Reading the training csv file
test = pd.read_csv('./test_input.csv')   # Reading the testing csv file

train.head(5)      # Printing the head of train

test.head(5)      # Printing the head of test

"""## Feature Engineering"""

ntrain = train.shape[0]        # Number of samples in train
ntest = test.shape[0]          # Number of samples in test
y_train = train['Target Variable (Discrete)'].values    # Setting up y_train
all_data = pd.concat((train, test)).reset_index(drop=True)          # Concatenating train and test into all_data
all_data.drop(['Target Variable (Discrete)'], axis=1, inplace=True) # Dropping the Target Variable column
print("all_data size is : {}".format(all_data.shape))               # Printing the overall shape

print(all_data.dtypes)         # Printing the types in all_data

# Analysing the values present in target values
targets = train['Target Variable (Discrete)'].value_counts()  # Using value_counts to get the counts of each values
print(targets)                                      # Printing them
sns.countplot(train['Target Variable (Discrete)'])  # Plotting these counts
plt.show()

"""## Handling the missing Data"""

# Finding the missing percentages ratio
all_data_na = (all_data.isnull().sum() / len(all_data)) * 100   # Detecting missing values  in all_data
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30] # Sorting the values
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})     # Creating a dataframe to put these missing values
missing_data.head(20)                                           # Printing the head

# Plotting these missing values percentages
f, ax = plt.subplots(figsize=(15, 12)) 
plt.xticks(rotation='90')
sns.barplot(x=all_data_na.index, y=all_data_na)
plt.xlabel('Features', fontsize=15)
plt.ylabel('Percent of missing values', fontsize=15)
plt.title('Percent missing data by feature', fontsize=15)

# Correlation map to see how features are correlated
corrmat = train.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)

# Filling the 'nan' in Feature 15,16,17,18 with 0's 
objects=['15','16','17','18']
for i in objects:
    all_data['Feature '+i]=all_data['Feature '+i].apply(pd.to_numeric, errors='coerce').fillna(0, downcast='infer')

# Filling the 'nan' in Feature 14,24,13,12,11,10 with their respective means
features = [14,24,13,12,11,10]
for i in features:    
    all_data['Feature '+str(i)] = all_data['Feature '+str(i)].fillna(all_data['Feature '+str(i)].mode()[0])
all_data['Feature 9']=all_data['Feature 9'].fillna(all_data['Feature 9'].mean())

# Again check remaining missing values if any  
all_data_na = (all_data.isnull().sum() / len(all_data)) * 100  # Detecting missing values  in all_data
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)  # Sorting the values
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})    # Creating a dataframe to put these missing values
missing_data.head()                                            # Printing the head

# Converting 5,6,7,8, features into category type
cat_features=['5','6','7','8']
for i in cat_features:
    all_data['Feature '+i+' (Discrete)']=all_data['Feature '+i+' (Discrete)'].astype('category')

print(all_data.dtypes)    # Printing the dtypes

def onehotencoding(feature,data):               # Function that does onehotencoding for the given column
    features=pd.get_dummies(data[feature], prefix=feature)
    return features

# Perfoming one hot encoding to 5,6,7,8 categorical features
Disc_5=onehotencoding("Feature 5 (Discrete)",all_data)
Disc_6=onehotencoding("Feature 6 (Discrete)",all_data)
Disc_7=onehotencoding("Feature 7 (Discrete)",all_data)
Disc_8=onehotencoding("Feature 8 (Discrete)",all_data)

# Dropping off the categorical column
features=["Feature 5 (Discrete)","Feature 6 (Discrete)","Feature 7 (Discrete)","Feature 8 (Discrete)"]
all_data.drop(features,axis=1,inplace=True)

# Concatinating the one hot encoded vectors
all_data = pd.concat([all_data, Disc_5], axis=1)   
all_data = pd.concat([all_data, Disc_6], axis=1)  
all_data = pd.concat([all_data, Disc_7], axis=1)  
all_data = pd.concat([all_data, Disc_8], axis=1)

all_data.dtypes               # Printing the dtypes

print(all_data.shape)        # Printing the shape of the final all_data

# Splitting train,test from all_data
train = all_data[:ntrain]
test = all_data[ntrain:]

print(train.shape)   # Printing the train shape

print(test.shape)   # Printing the test shape

"""## Training"""

# Importing models needed for training part
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error,SCORERS
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint
import xgboost as xgb
import lightgbm as lgb

# Validation function
n_folds = 5    # 5 folds validation

def score_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)    # Splitting train into 5 folds
    acc= cross_val_score(model, train.values, y_train, scoring="accuracy", cv = kf)  # Evaluating score by cross-validation
    return acc    # Returing the accuracies obtained

# GBoost classifier
GBoost = GradientBoostingClassifier(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10, random_state =5)

# XGB classfier 
model_xgb = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, 
                             learning_rate=0.05, max_depth=3, 
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213,
                             random_state =7, nthread = -1)

# LGBM Classifier
model_lgb = lgb.LGBMClassifier(objective='multiclass',num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin = 55, bagging_fraction = 0.8,
                              bagging_freq = 5, feature_fraction = 0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)

# Random forest
rf = RandomForestClassifier(random_state = 42)
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
max_features = ['auto', 'sqrt']      # Number of features to consider at every split
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]  # Maximum number of levels in tree
max_depth.append(None)
min_samples_split = [2, 5, 10]       # Minimum number of samples required to split a node
min_samples_leaf = [1, 2, 4]         # Minimum number of samples required at each leaf node
bootstrap = [True, False]            # Method of selecting samples for training each tree
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

rf = RandomForestClassifier()       # Number of trees in random forest
# Random search of parameters, using 3 fold cross validation, 
# Search across 100 different combinations, and use all available cores
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(train, y_train)       # Fit the random search model
print(rf_random.best_params_)       # Printing the params

"""# Defining Stacking Strategy (Taken from [Faron](https://www.kaggle.com/getting-started/18153#post103381) )

In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model. 

The procedure, for the training part, may be described as follows:


1. Split the total training set into two disjoint sets (here **train** and .**holdout** )

2. Train several base models on the first part (**train**)

3. Test these base models on the second part (**holdout**)

4. Use the predictions from 3)  (called  out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs  to train a higher level learner called **meta-model**.

The first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration,  we train every base model on 4 folds and predict on the remaining fold (holdout fold). 

So, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as 
new feature to train our meta-model in the step 4.

For the prediction part , We average the predictions of  all base models on the test data  and used them as **meta-features**  on which, the final prediction is done with the meta-model.

![Faron](http://i.imgur.com/QBuDOjs.jpg)
"""

# Function for stacking models
class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, base_models, meta_model, n_folds=5): # Initialising parameters
        self.base_models = base_models
        self.meta_model = meta_model
        self.n_folds = n_folds
   
    def fit(self, X, y):     # Fit on the models
        self.base_models_ = [list() for x in self.base_models]
        self.meta_model_ = clone(self.meta_model)
        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)
        # Train base models then create out-of-fold predictions that are needed to train the cloned meta-model
        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))
        for i, model in enumerate(self.base_models):
            for train_index, holdout_index in kfold.split(X, y):
                instance = clone(model)
                self.base_models_[i].append(instance)
                instance.fit(X[train_index], y[train_index])  # Fitting
                y_pred = instance.predict(X[holdout_index])   # Predict
                out_of_fold_predictions[holdout_index, i] = y_pred
                
        self.meta_model_.fit(out_of_fold_predictions, y)      # Now train the cloned  meta-model using the out-of-fold predictions as new feature
        return self  
    
    def predict(self, X): # Do the predictions of all base models on the test data and use the averaged predictions as Meta-features for the final prediction which is done by the meta-model
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict_proba(meta_features)

# Giving xgb model as meta model
# Giving lgb,Gboost as base models as they are weak learners
stacked_averaged_models = StackingAveragedModels(base_models = (model_lgb, GBoost), meta_model = model_xgb)

"""### Scores"""

score = score_cv(model_xgb)  # Finding the score of xgb model
print("Xgboost score: {:.4f} ({:.4f})\n".format(score.mean(), score.std())) # Printing the average of the 5 fold accuracies

score = score_cv(model_lgb)  # Finding the score of lgb model
print("LGB score: {:.4f} ({:.4f})\n".format(score.mean(), score.std())) # Printing the average of the 5 fold accuracies

score = score_cv(GBoost)     # Finding the score of GBoost
print("LGB score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))  # Printing the average of the 5 fold accuracies

score = score_cv(rf_random.best_estimator_)   # Finding the score of GBoost
print("Random Forest score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))  # Printing the average of the 5 fold accuracies

stacked_averaged_models.fit(train.values, y_train)  # Fitting the train values on the model
stacked_train_pred = stacked_averaged_models.predict(train.values) # Train predictions
stacked_pred = (stacked_averaged_models.predict(test.values))  # Predicting on test set and storing them
stacked_model = "stacked.pkl"  
with open(stacked_model, 'wb') as file:             # Saving the weights file
    pickle.dump(stacked_averaged_models, file)

model_xgb.fit(train, y_train)                 # Fitting the train values on the model
xgb_train_pred = model_xgb.predict(train)     # Train predictions
xgb_pred =(model_xgb.predict_proba(test))     # Predicting on test set and storing them
xgb_model = "xgb.pkl"  
with open(xgb_model, 'wb') as file:           # Saving the weights file
    pickle.dump(model_xgb, file)

model_lgb.fit(train, y_train)                 # Fitting the train values on the model
lgb_train_pred = model_lgb.predict(train)     # Train predictions
lgb_pred=model_lgb.predict_proba(test)        # Predicting on test set and storing them
lgb_model = "lgb.pkl"  
with open(lgb_model, 'wb') as file:           # Saving the weights file
    pickle.dump(model_lgb, file)

GBoost.fit(train, y_train)                    # Fitting the train values on the model
GBoost_train_pred = GBoost.predict(train)     # Train predictions
GBoost_pred = GBoost.predict_proba(test.values) # Predicting on test set and storing them
Gboost_model= "Gboost.pkl"  
with open(Gboost_model, 'wb') as file:        # Saving the weights file
    pickle.dump(GBoost, file)

rf_random.best_estimator_.fit(train, y_train)  # Fitting the train values on the model
rf_random.best_estimator_train_pred = rf_random.best_estimator_.predict(train)       # Train predictions
rf_random.best_estimator_pred = rf_random.best_estimator_.predict_proba(test.values) # Predicting on test set and storing them
rf_model= "rf.pkl"  
with open(rf_model, 'wb') as file:             # Saving the weights file
    pickle.dump(rf_random.best_estimator_, file)


"""## Loading Weights"""

Gboost= "./Gboost.pkl"  # Loading weights from weights file
with open(Gboost, 'rb') as file:  
    Gboost_model = pickle.load(file)
GBoost_pred = Gboost_model.predict_proba(test.values)  # Finding the prediction probabilities for test values

lgb= "./lgb.pkl"       # Loading weights from weights file
with open(lgb, 'rb') as file:  
    lgb_model = pickle.load(file)
lgb_pred = lgb_model.predict_proba(test.values)   # Finding the prediction probabilities for test values

rf="./rf.pkl"          # Loading weights from weights file
with open(rf, 'rb') as file:  
    rf_model = pickle.load(file)
rf_pred = rf_model.predict_proba(test.values)     # Finding the prediction probabilities for test values

stacked="./stacked.pkl"  # Loading weights from weights file
with open(stacked, 'rb') as file:  
    stacked_model = pickle.load(file)
stacked_pred = stacked_model.predict(test.values)  # Finding the prediction probabilities for test values

xgb="./xgb.pkl"          # Loading weights from weights file
with open(xgb, 'rb') as file:  
    xgb_model = pickle.load(file)
xgb_pred = xgb_model.predict_proba(test.values)  # Finding the prediction probabilities for test values

"""## Prediction Weights
Based on the score accuracies for each models we have given weights to each model 

Random Forest - 0.5

Stacked Model - 0.3

Xgb           - 0.1

GBoost        - 0.05

LGBM          - 0.05
"""

final_prob = np.multiply(rf_pred,0.5)+np.multiply(stacked_pred,0.3)+np.multiply(xgb_pred,0.1)+np.multiply(GBoost_pred,0.05)+np.multiply(lgb_pred,0.05)
lst=[]
for i in range(len(test)):
    lst.append([i+1,np.argmax(final_prob[i])])
ensemble = pd.DataFrame(lst)
ensemble.columns=["Id","Category"]
ensemble.to_csv('test_output.csv',index=False)
print(ensemble.head(5))